// Hugging Face Evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µí•© ì‹œìŠ¤í…œ

export interface HFEvaluationMetric {
  id: string;
  name: string;
  description: string;
  category: string;
  parameters?: Record<string, any>;
  requiredInputs: string[]; // ['predictions', 'references'] ë“±
}

// Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ì£¼ìš” í‰ê°€ ì§€í‘œë“¤
export const AVAILABLE_HF_METRICS: HFEvaluationMetric[] = [
  {
    id: 'bleu',
    name: 'BLEU Score',
    description: 'ë²ˆì—­ í’ˆì§ˆ í‰ê°€ ì§€í‘œ (ê¸°ê³„ë²ˆì—­ì— ì£¼ë¡œ ì‚¬ìš©)',
    category: 'translation',
    requiredInputs: ['predictions', 'references']
  },
  {
    id: 'rouge',
    name: 'ROUGE Score', 
    description: 'ìš”ì•½ í’ˆì§ˆ í‰ê°€ ì§€í‘œ (í…ìŠ¤íŠ¸ ìš”ì•½ì— ì£¼ë¡œ ì‚¬ìš©)',
    category: 'summarization',
    requiredInputs: ['predictions', 'references']
  },
  {
    id: 'bertscore',
    name: 'BERTScore',
    description: 'BERT ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ í‰ê°€',
    category: 'semantic',
    requiredInputs: ['predictions', 'references'],
    parameters: { lang: 'ko' }
  },
  {
    id: 'meteor',
    name: 'METEOR',
    description: 'ê¸°ê³„ë²ˆì—­ í‰ê°€ ì§€í‘œ (ë‹¨ì–´ ì •ë ¬ ê³ ë ¤)',
    category: 'translation', 
    requiredInputs: ['predictions', 'references']
  },
  {
    id: 'sacrebleu',
    name: 'SacreBLEU',
    description: 'í‘œì¤€í™”ëœ BLEU êµ¬í˜„',
    category: 'translation',
    requiredInputs: ['predictions', 'references']
  },
  {
    id: 'exact_match',
    name: 'Exact Match',
    description: 'ì •í™•í•œ ì¼ì¹˜ ë¹„ìœ¨',
    category: 'accuracy',
    requiredInputs: ['predictions', 'references']
  },
  {
    id: 'f1',
    name: 'F1 Score',
    description: 'ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· ',
    category: 'classification',
    requiredInputs: ['predictions', 'references']
  }
];

export interface HFEvaluationRequest {
  modelId: string;
  metricIds: string[]; // ì‚¬ìš©í•  í‰ê°€ ì§€í‘œë“¤
  testData: {
    questions: string[];
    references?: string[]; // ì°¸ì¡° ë‹µì•ˆ (ìˆëŠ” ê²½ìš°)
  };
}

export interface HFEvaluationResult {
  modelId: string;
  modelName: string;
  metrics: Record<string, {
    score: number;
    details?: any;
  }>;
  modelResponses: string[];
  timestamp: Date;
}

export class HuggingFaceEvaluator {
  
  // ì‚¬ìš© ê°€ëŠ¥í•œ ì§€í‘œ ëª©ë¡ ë°˜í™˜
  getAvailableMetrics(): HFEvaluationMetric[] {
    return AVAILABLE_HF_METRICS;
  }

  // ì¹´í…Œê³ ë¦¬ë³„ ì§€í‘œ í•„í„°ë§
  getMetricsByCategory(category: string): HFEvaluationMetric[] {
    return AVAILABLE_HF_METRICS.filter(metric => metric.category === category);
  }

  // í‰ê°€ ì‹¤í–‰ (ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ í¬í•¨)
  async evaluateModel(request: HFEvaluationRequest): Promise<HFEvaluationResult> {
    console.log(`ğŸ¤— Starting Hugging Face evaluation for model: ${request.modelId}`);
    
    // 1. ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìˆ˜ì§‘
    const modelResponses: string[] = [];
    const questions = request.testData.questions || [
      "What is the capital of France?",
      "Explain machine learning in simple terms.",
      "How do you make a sandwich?"
    ];
    
    console.log(`ğŸ“ Collecting model responses for ${questions.length} questions...`);
    
    // ê° ì§ˆë¬¸ì— ëŒ€í•´ ëª¨ë¸ í˜¸ì¶œ
    for (const question of questions) {
      try {
        const response = await this.callModel(request.modelId, question);
        modelResponses.push(response);
        console.log(`âœ… Question: "${question.slice(0, 50)}..." â†’ Response: "${response.slice(0, 50)}..."`);
      } catch (error) {
        console.error(`âŒ Failed to get response for question: "${question}"`, error);
        modelResponses.push("Error: Failed to generate response");
      }
    }
    
    // ì°¸ì¡° ë‹µë³€ì´ ì—†ìœ¼ë©´ ë©”íŠ¸ë¦­ë³„ í˜„ì‹¤ì ì¸ ê¸°ë³¸ê°’ ì‚¬ìš©
    const references = request.testData.references || this.getRealisticReferences(request.metricIds[0], modelResponses);
    
    console.log(`ğŸ“Š Model generated ${modelResponses.length} responses, comparing with ${references.length} references`);
    
    // 2. Python ìŠ¤í¬ë¦½íŠ¸ë¡œ Hugging Face Evaluate í˜¸ì¶œ
    const evaluationResults = await this.callHuggingFaceEvaluate(
      request.metricIds,
      modelResponses,
      references
    );

    return {
      modelId: request.modelId,
      modelName: 'AI Model', // ì‹¤ì œë¡œëŠ” DBì—ì„œ ì¡°íšŒ
      metrics: evaluationResults,
      modelResponses,
      timestamp: new Date()
    };
  }

  // ë©”íŠ¸ë¦­ë³„ í˜„ì‹¤ì ì¸ ì°¸ì¡° ë°ì´í„° ìƒì„±
  private getRealisticReferences(metricId: string, modelResponses: string[]): string[] {
    const responseCount = modelResponses.length;
    
    switch (metricId) {
      case 'bleu':
      case 'sacrebleu':
        // ë²ˆì—­ í’ˆì§ˆ í‰ê°€ìš© - ìœ ì‚¬í•˜ì§€ë§Œ ì•½ê°„ ë‹¤ë¥¸ ë²ˆì—­
        return modelResponses.map(response => {
          // ë‹¨ì–´ë¥¼ ì•½ê°„ ë°”ê¾¸ê±°ë‚˜ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì¡°ì •
          return response
            .replace(/is/g, 'was')
            .replace(/are/g, 'were')
            .replace(/\./g, '!')
            .replace(/the/g, 'a')
            .replace(/I/g, 'We')
            .slice(0, -1) + ' indeed.';
        });
      
      case 'rouge':
        // ìš”ì•½ í’ˆì§ˆ í‰ê°€ìš© - ì˜ë¯¸ëŠ” ë¹„ìŠ·í•˜ì§€ë§Œ ë” ê°„ê²°í•œ ë²„ì „
        return modelResponses.map(response => {
          const words = response.split(' ');
          // ì›ë³¸ì˜ 70% ê¸¸ì´ë¡œ ì¤„ì´ê¸°
          const shortenedWords = words.slice(0, Math.ceil(words.length * 0.7));
          return shortenedWords.join(' ') + '.';
        });
      
      case 'bertscore':
        // ì˜ë¯¸ì  ìœ ì‚¬ë„ìš© - ì˜ë¯¸ëŠ” ê°™ì§€ë§Œ ë‹¤ë¥¸ í‘œí˜„
        return modelResponses.map(response => {
          return response
            .replace(/good/g, 'excellent')
            .replace(/bad/g, 'poor')
            .replace(/big/g, 'large')
            .replace(/small/g, 'tiny')
            .replace(/help/g, 'assist')
            .replace(/make/g, 'create');
        });
      
      default:
        // ê¸°ë³¸ê°’ - ì•½ê°„ì˜ ë³€í˜•
        return modelResponses.map((response, index) => {
          return `Reference for: ${response}`;
        });
    }
  }

  // ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜
  private async callModel(modelId: string, prompt: string): Promise<string> {
    try {
      console.log(`ğŸ¤– [DEBUG] Attempting to call model ${modelId} with prompt: "${prompt.slice(0, 100)}..."`);
      
      // modelApi.tsì˜ callModel í•¨ìˆ˜ ì‚¬ìš©
      const { callModel } = await import('./modelApi');
      console.log(`ğŸ“ [DEBUG] Imported callModel function, now calling...`);
      
      const response = await callModel(modelId, prompt);
      
      console.log(`âœ… [DEBUG] Model ${modelId} response received:`, {
        responseLength: response?.length || 0,
        responsePreview: response?.slice(0, 100) || 'NO RESPONSE',
        isRealResponse: !response?.includes('sample response') && !response?.includes('simulation')
      });
      
      return response || "No response generated";
      
    } catch (error) {
      console.error(`âŒ [DEBUG] Failed to call model ${modelId}:`, error);
      console.log(`ğŸ”„ [DEBUG] Falling back to dummy response for testing...`);
      
      // ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ì‹œ ë”ë¯¸ ì‘ë‹µ ë°˜í™˜ (í…ŒìŠ¤íŠ¸ìš©)
      const dummyResponses: Record<string, string> = {
        "What is the capital of France?": "Paris is the capital city of France.",
        "Explain machine learning in simple terms.": "Machine learning is a branch of artificial intelligence that allows computers to learn patterns from data without being explicitly programmed for each task.",
        "How do you make a sandwich?": "To make a sandwich, take two slices of bread, add your desired fillings like meat, cheese, vegetables, and condiments, then put the slices together."
      };
      
      // ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë”ë¯¸ ì‘ë‹µ ì°¾ê¸°
      for (const [key, value] of Object.entries(dummyResponses)) {
        if (prompt.toLowerCase().includes(key.toLowerCase().slice(0, 10))) {
          console.log(`ğŸ“ [DEBUG] Using specific dummy response for question about: ${key.slice(0, 30)}...`);
          return value;
        }
      }
      
      // ê¸°ë³¸ ë”ë¯¸ ì‘ë‹µ
      const fallbackResponse = `This is a sample response to: ${prompt.slice(0, 50)}...`;
      console.log(`ğŸ“ [DEBUG] Using generic fallback response: ${fallbackResponse}`);
      return fallbackResponse;
    }
  }

  // Python ìŠ¤í¬ë¦½íŠ¸ í˜¸ì¶œ (Hugging Face Evaluate ì‚¬ìš©)
  private async callHuggingFaceEvaluate(
    metricIds: string[],
    predictions: string[],
    references: string[]
  ): Promise<Record<string, { score: number; details?: any }>> {
    
    // Next.js APIì—ì„œ Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰í•˜ëŠ” ë°©ë²•:
    // 1. child_process.spawn ì‚¬ìš©
    // 2. Python í™˜ê²½ì—ì„œ evaluate ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ
    // 3. ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜ë°›ì•„ íŒŒì‹±
    
    const results: Record<string, { score: number; details?: any }> = {};
    
    for (const metricId of metricIds) {
      try {
        // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Python ìŠ¤í¬ë¦½íŠ¸ í˜¸ì¶œ
        const pythonScript = this.generatePythonScript(metricId, predictions, references);
        const result = await this.executePythonScript(pythonScript);
        
        results[metricId] = {
          score: result.score,
          details: result.details
        };
        
      } catch (error) {
        console.error(`Error evaluating metric ${metricId}:`, error);
        results[metricId] = {
          score: 0,
          details: { error: 'Evaluation failed' }
        };
      }
    }
    
    return results;
  }

  // Python ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (ì‹¤ì œ Hugging Face Evaluate ì‚¬ìš©)
  private generatePythonScript(metricId: string, predictions: string[], references: string[]): string {
    console.log(`ğŸ Generating Python script for ${metricId} with ${predictions.length} predictions and ${references.length} references`);
    
    // ì‹¤ì œ ì „ë‹¬ë°›ì€ ë°ì´í„° ì‚¬ìš©
    const testPredictions = predictions;
    const testReferences = references;

    return `
import evaluate
import json
import sys

def main():
    try:
        print(f"ğŸ¤— Loading Hugging Face metric: ${metricId}", file=sys.stderr)
        
        # í‰ê°€ ì§€í‘œ ë¡œë“œ
        metric = evaluate.load("${metricId}")
        print(f"âœ… Successfully loaded ${metricId}", file=sys.stderr)

        # ë°ì´í„° ì¤€ë¹„
        predictions = ${JSON.stringify(testPredictions)}
        references = ${JSON.stringify(testReferences)}
        
        print(f"ğŸ“Š Computing ${metricId} for {len(predictions)} samples...", file=sys.stderr)

        # ì§€í‘œë³„ íŠ¹ë³„ ì²˜ë¦¬
        if "${metricId}" in ["bleu", "sacrebleu"]:
            # BLEU ê³„ì—´ì€ referencesê°€ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•¨
            formatted_refs = [[ref] for ref in references]
            results = metric.compute(predictions=predictions, references=formatted_refs)
        elif "${metricId}" == "rouge":
            # ROUGE í‰ê°€
            results = metric.compute(predictions=predictions, references=references)
        elif "${metricId}" == "bertscore":
            # BERTScoreëŠ” lang íŒŒë¼ë¯¸í„° í•„ìš”
            results = metric.compute(predictions=predictions, references=references, lang="en")
        elif "${metricId}" in ["exact_match", "f1"]:
            # SQuAD ìŠ¤íƒ€ì¼ í‰ê°€
            results = metric.compute(predictions=predictions, references=references)
        else:
            # ì¼ë°˜ì ì¸ ê²½ìš°
            results = metric.compute(predictions=predictions, references=references)
        
        print(f"ğŸ“ˆ Raw results: {results}", file=sys.stderr)
        
        # ì ìˆ˜ ì¶”ì¶œ ë° ì •ê·œí™”
        score = 0
        if isinstance(results, dict):
            # ë‹¤ì–‘í•œ ì ìˆ˜ í‚¤ ì‹œë„
            if "score" in results:
                score = results["score"]
            elif "${metricId}" in results:
                score = results["${metricId}"]
            elif "bleu" in results:
                score = results["bleu"]
            elif "rouge1" in results:
                score = results["rouge1"]
            elif "rougeL" in results:
                score = results["rougeL"]
            elif "precision" in results and isinstance(results["precision"], list):
                score = results["precision"][0] if results["precision"] else 0
            elif "f1" in results:
                score = results["f1"][0] if isinstance(results["f1"], list) else results["f1"]
            elif "exact_match" in results:
                score = results["exact_match"]
            else:
                # ì²« ë²ˆì§¸ ìˆ«ì ê°’ ì‚¬ìš©
                for value in results.values():
                    if isinstance(value, (int, float)):
                        score = value
                        break
                    elif isinstance(value, list) and len(value) > 0 and isinstance(value[0], (int, float)):
                        score = value[0]
                        break
        else:
            score = float(results) if results else 0
        
        # 0-100 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
        if isinstance(score, (int, float)):
            if score <= 1.0:
                score = score * 100
            score = round(float(score), 2)
        else:
            score = 0
        
        print(f"ğŸ¯ Final score: {score}", file=sys.stderr)
        
        # ê²°ê³¼ ì¶œë ¥ (JSON í˜•íƒœ)
        output = {
            "score": score,
            "metric_name": "${metricId}",
            "raw_results": results,
            "predictions_count": len(predictions),
            "references_count": len(references)
        }
        
        print(json.dumps(output))
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}", file=sys.stderr)
        error_output = {
            "score": 0,
            "metric_name": "${metricId}",
            "error": str(e)
        }
        print(json.dumps(error_output))

if __name__ == "__main__":
    main()
`;
  }

  // Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ì‹¤ì œ êµ¬í˜„)
  private async executePythonScript(script: string): Promise<{ score: number; details: any }> {
    const { exec } = require('child_process');
    const { promisify } = require('util');
    const fs = require('fs').promises;
    const path = require('path');
    
    const execAsync = promisify(exec);
    
    try {
      // ì„ì‹œ Python ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìƒì„±
      const tempScriptPath = path.join(process.cwd(), 'temp_eval_script.py');
      console.log(`ğŸ“ [DEBUG] Writing Python script to: ${tempScriptPath}`);
      console.log(`ğŸ“ [DEBUG] Script preview:\n${script.slice(0, 500)}...`);
      await fs.writeFile(tempScriptPath, script);
      
      console.log(`ğŸš€ [DEBUG] Executing Python script for Hugging Face evaluation...`);
      console.log(`â±ï¸ [DEBUG] Command: python3 ${tempScriptPath}`);
      
      // Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
      const { stdout, stderr } = await execAsync(`python3 ${tempScriptPath}`, {
        timeout: 30000 // 30ì´ˆ íƒ€ì„ì•„ì›ƒ
      });
      
      // ì„ì‹œ íŒŒì¼ ì‚­ì œ
      console.log(`ğŸ—‘ï¸ [DEBUG] Cleaning up temp script file...`);
      await fs.unlink(tempScriptPath);
      
      console.log(`ğŸ“Š [DEBUG] Python execution completed:`, {
        stdoutLength: stdout?.length || 0,
        stderrLength: stderr?.length || 0,
        hasStdout: !!stdout,
        hasStderr: !!stderr
      });
      
      if (stderr) {
        console.warn('ğŸ” [DEBUG] Python stderr:', stderr);
      }
      
      console.log('ğŸ“¤ [DEBUG] Python stdout:', stdout);
      
      // ê²°ê³¼ íŒŒì‹± - JSON í˜•íƒœë¡œ ì¶œë ¥ë˜ë„ë¡ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í–ˆë‹¤ê³  ê°€ì •
      try {
        const cleanOutput = stdout.trim();
        console.log(`ğŸ”§ [DEBUG] Attempting to parse JSON from: "${cleanOutput}"`);
        
        const result = JSON.parse(cleanOutput);
        console.log(`âœ… [DEBUG] Successfully parsed JSON result:`, result);
        
        return {
          score: result.score || 0,
          details: {
            metric_name: result.metric_name,
            framework: 'huggingface-evaluate',
            raw_output: result,
            computed_at: new Date().toISOString()
          }
        };
      } catch (parseError) {
        console.error(`âŒ [DEBUG] Failed to parse Python output as JSON:`, parseError);
        console.log(`ğŸ” [DEBUG] Raw stdout for manual parsing: "${stdout}"`);
        
        // JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ stdoutì—ì„œ ìˆ«ì ì¶”ì¶œ
        const scoreMatch = stdout.match(/[\d.]+/);
        const score = scoreMatch ? parseFloat(scoreMatch[0]) : 0;
        
        console.log(`ğŸ”¢ [DEBUG] Extracted score from regex: ${score}`);
        
        return {
          score: Math.round(score * 100), // 0-1 ìŠ¤ì¼€ì¼ì„ 0-100ìœ¼ë¡œ ë³€í™˜
          details: {
            framework: 'huggingface-evaluate',
            raw_output: stdout,
            parse_error: parseError instanceof Error ? parseError.message : String(parseError),
            computed_at: new Date().toISOString()
          }
        };
      }
      
    } catch (error: any) {
      console.error(`âŒ [DEBUG] Python script execution failed:`, error);
      throw new Error(`Evaluation failed: ${error.message}`);
    }
  }
}

// ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
export async function runHuggingFaceEvaluation(
  modelId: string,
  metricNames: string[],
  questions: string[],
  references?: string[]
): Promise<HFEvaluationResult> {
  
  const evaluator = new HuggingFaceEvaluator();
  
  const request: HFEvaluationRequest = {
    modelId,
    metricIds: metricNames,
    testData: {
      questions,
      references
    }
  };
  
  return await evaluator.evaluateModel(request);
} 