// Ïã§Ï†ú Î™®Îç∏ API Ìò∏Ï∂úÏùÑ ÏúÑÌïú Ïú†Ìã∏Î¶¨Ìã∞

interface ModelConfig {
  provider: 'openai' | 'anthropic' | 'google' | 'huggingface';
  modelName: string;
  apiKey: string;
  endpoint?: string;
}

interface ModelResponse {
  text: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export class ModelAPIClient {
  private config: ModelConfig;

  constructor(config: ModelConfig) {
    this.config = config;
  }

  async generateResponse(prompt: string): Promise<ModelResponse> {
    switch (this.config.provider) {
      case 'openai':
        return this.callOpenAI(prompt);
      case 'anthropic':
        return this.callAnthropic(prompt);
      case 'google':
        return this.callGoogle(prompt);
      case 'huggingface':
        return this.callHuggingFace(prompt);
      default:
        throw new Error(`Unsupported provider: ${this.config.provider}`);
    }
  }

  private async callOpenAI(prompt: string): Promise<ModelResponse> {
    try {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.config.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.config.modelName,
          messages: [
            {
              role: 'user',
              content: prompt
            }
          ],
          max_tokens: 500,
          temperature: 0.7
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`OpenAI API Error Details:`, {
          status: response.status,
          statusText: response.statusText,
          modelName: this.config.modelName,
          error: errorText
        });
        throw new Error(`OpenAI API error: ${response.statusText} - ${errorText}`);
      }

      const data = await response.json();
      
      return {
        text: data.choices[0].message.content,
        usage: data.usage
      };
    } catch (error) {
      console.error('OpenAI API call failed:', error);
      throw error;
    }
  }

  private async callAnthropic(prompt: string): Promise<ModelResponse> {
    try {
      const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
          'x-api-key': this.config.apiKey,
          'Content-Type': 'application/json',
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
          model: this.config.modelName,
          max_tokens: 500,
          messages: [
            {
              role: 'user',
              content: prompt
            }
          ],
          system: "You are a helpful AI assistant."
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`Anthropic API Error Details:`, {
          status: response.status,
          statusText: response.statusText,
          modelName: this.config.modelName,
          error: errorText
        });
        throw new Error(`Anthropic API error: ${response.statusText} - ${errorText}`);
      }

      const data = await response.json();
      
      return {
        text: data.content[0].text,
        usage: data.usage
      };
    } catch (error) {
      console.error('Anthropic API call failed:', error);
      throw error;
    }
  }

  private async callGoogle(prompt: string): Promise<ModelResponse> {
    try {
      const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${this.config.modelName}:generateContent?key=${this.config.apiKey}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [
            {
              parts: [
                {
                  text: prompt
                }
              ]
            }
          ],
          generationConfig: {
            maxOutputTokens: 500,
            temperature: 0.7
          }
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`Google API Error Details:`, {
          status: response.status,
          statusText: response.statusText,
          modelName: this.config.modelName,
          error: errorText
        });
        throw new Error(`Google API error: ${response.statusText} - ${errorText}`);
      }

      const data = await response.json();
      
      return {
        text: data.candidates[0].content.parts[0].text
      };
    } catch (error) {
      console.error('Google API call failed:', error);
      throw error;
    }
  }

  private async callHuggingFace(prompt: string): Promise<ModelResponse> {
    try {
      const response = await fetch(
        this.config.endpoint || `https://api-inference.huggingface.co/models/${this.config.modelName}`,
        {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${this.config.apiKey}`,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            inputs: prompt,
            parameters: {
              max_new_tokens: 500,
              temperature: 0.7,
              return_full_text: false
            }
          })
        }
      );

      if (!response.ok) {
        throw new Error(`HuggingFace API error: ${response.statusText}`);
      }

      const data = await response.json();
      
      return {
        text: Array.isArray(data) ? data[0].generated_text : data.generated_text
      };
    } catch (error) {
      console.error('HuggingFace API call failed:', error);
      throw error;
    }
  }
}

// Î™®Îç∏ ÏÑ§Ï†ïÏùÑ Í∞ÄÏ†∏Ïò§Îäî Ìï®Ïàò
export async function getModelConfig(modelId: string): Promise<ModelConfig | null> {
  console.log(`üîç Getting config for model: ${modelId}`);
  
  // ÏßÄÏ†ïÎêú 3Í∞ú UUIDÏóê ÎåÄÌïú ÏßÅÏ†ë Îß§Ìïë
  const uuidToConfigMap: { [key: string]: ModelConfig } = {
    'cb7d2bb8-049c-4271-99a2-bffedebe2487': {
      provider: 'openai',
      modelName: 'gpt-4o',
      apiKey: process.env.OPENAI_API_KEY || '',
    },
    '603d268f-d984-43b6-a85e-445bdd955061': {
      provider: 'anthropic',
      modelName: 'claude-3-opus-20240229',
      apiKey: process.env.ANTHROPIC_API_KEY || '',
    },
    '3e72f00e-b450-4dff-812e-a013c4cca457': {
      provider: 'google',
      modelName: 'gemini-2.0-flash',
      apiKey: process.env.GOOGLE_API_KEY || '',
    }
  };

  // UUID Í∏∞Î∞ò ÏÑ§Ï†ï ÌôïÏù∏
  if (uuidToConfigMap[modelId]) {
    console.log(`‚úÖ Found direct UUID mapping for ${modelId}`);
    return uuidToConfigMap[modelId];
  }

  // UUID ÌòïÌÉúÏù¥ÏßÄÎßå Îß§ÌïëÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Ï°∞Ìöå
  if (modelId.includes('-')) {
    try {
      const response = await fetch(`${process.env.NEXT_PUBLIC_SITE_URL || 'http://localhost:3000'}/api/models`);
      if (response.ok) {
        const models = await response.json();
        const model = models.find((m: any) => m.id === modelId);
        if (model) {
          console.log(`üìã Found model in database: ${model.name}`);
          // Î™®Îç∏ Ïù¥Î¶ÑÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÑ§Ï†ï Îß§Ìïë
          const normalizedName = model.name.toLowerCase().replace(/[^a-z0-9]/g, '');
          if (normalizedName.includes('gpt4') || normalizedName.includes('gpt-4') || normalizedName.includes('gpt4')) {
            return {
              provider: 'openai',
              modelName: 'gpt-4o',
              apiKey: process.env.OPENAI_API_KEY || '',
            };
          } else if (normalizedName.includes('claude')) {
            return {
              provider: 'anthropic',
              modelName: 'claude-3-opus-20240229',
              apiKey: process.env.ANTHROPIC_API_KEY || '',
            };
          } else if (normalizedName.includes('gemini') || normalizedName.includes('google')) {
            return {
              provider: 'google',
              modelName: 'gemini-2.0-flash',
              apiKey: process.env.GOOGLE_API_KEY || '',
            };
          }
        }
      }
    } catch (error) {
      console.error('Error fetching model info:', error);
    }
  }
  
  // Î¨∏ÏûêÏó¥ Í∏∞Î∞ò Î™®Îç∏ ID ÏßÄÏõê (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
  const modelConfigs: { [key: string]: ModelConfig } = {
    'gpt-4-turbo': {
      provider: 'openai',
      modelName: 'gpt-4o',
      apiKey: process.env.OPENAI_API_KEY || '',
    },
    'claude-3-opus': {
      provider: 'anthropic',
      modelName: 'claude-3-opus-20240229',
      apiKey: process.env.ANTHROPIC_API_KEY || '',
    },
    'gemini-2.0-flash': {
      provider: 'google',
      modelName: 'gemini-2.0-flash',
      apiKey: process.env.GOOGLE_API_KEY || '',
    }
  };

  return modelConfigs[modelId] || null;
}

// Ïã§Ï†ú Î™®Îç∏ Ìò∏Ï∂ú Ìï®Ïàò
export async function callModel(modelId: string, prompt: string): Promise<string> {
  console.log(`üöÄ Calling model: ${modelId}`);
  const config = await getModelConfig(modelId);
  
  if (!config) {
    console.warn(`‚ùå No configuration found for model: ${modelId}, using fallback`);
    // ÏÑ§Ï†ïÏù¥ ÏóÜÏúºÎ©¥ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏùëÎãµ Î∞òÌôò
    return await getFallbackResponse(modelId, prompt);
  }

  if (!config.apiKey) {
    console.warn(`üîë No API key found for model: ${modelId}, using fallback`);
    return await getFallbackResponse(modelId, prompt);
  }

  console.log(`‚úÖ API key found for ${config.provider}, attempting real API call...`);
  
  try {
    const client = new ModelAPIClient(config);
    const response = await client.generateResponse(prompt);
    console.log(`‚úÖ Successfully called ${modelId} API`);
    return response.text;
  } catch (error) {
    console.error(`‚ùå Error calling model ${modelId}:`, error);
    // API Ìò∏Ï∂ú Ïã§Ìå®Ïãú Ìè¥Î∞± ÏùëÎãµ Î∞òÌôò
    return await getFallbackResponse(modelId, prompt);
  }
}

// Ìè¥Î∞± ÏùëÎãµ (ÏãúÎÆ¨Î†àÏù¥ÏÖò)
async function getFallbackResponse(modelId: string, prompt: string): Promise<string> {
  console.log(`üîÑ Using fallback simulation for model: ${modelId}`);
  
  // Ïã§Ï†ú API Ìò∏Ï∂úÍ≥º ÎπÑÏä∑Ìïú ÏßÄÏó∞ ÏãúÍ∞Ñ Ï∂îÍ∞Ä (2-4Ï¥à)
  const delay = 2000 + Math.random() * 2000;
  await new Promise(resolve => setTimeout(resolve, delay));
  
  const responses: { [key: string]: string[] } = {
    'gpt-4': [
      "ÏÇºÍ∞ÅÌòïÏùÄ ÏÑ∏ Í∞úÏùò Î≥ÄÍ≥º ÏÑ∏ Í∞úÏùò Íº≠ÏßìÏ†êÏùÑ Í∞ÄÏßÑ Îã§Í∞ÅÌòïÏûÖÎãàÎã§. ÏÇºÍ∞ÅÌòïÏùò ÎÇ¥Í∞ÅÏùò Ìï©ÏùÄ Ìï≠ÏÉÅ 180ÎèÑÏûÖÎãàÎã§.",
      "17 + 25Î•º Í≥ÑÏÇ∞ÌïòÎ©¥ 42ÏûÖÎãàÎã§. ÏùºÏùò ÏûêÎ¶¨ 7 + 5 = 12Ïù¥ÎØÄÎ°ú 1ÏùÑ Ïò¨Î¶ºÌïòÏó¨ Ïã≠Ïùò ÏûêÎ¶¨Îäî 1 + 1 + 2 = 4Í∞Ä Îê©ÎãàÎã§.",
      "ÏßÅÏÇ¨Í∞ÅÌòïÏùò ÎëòÎ†àÎäî (Í∞ÄÎ°ú + ÏÑ∏Î°ú) √ó 2Î°ú Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÏäµÎãàÎã§. Í∞ÄÎ°úÍ∞Ä 7cm, ÏÑ∏Î°úÍ∞Ä 4cmÏù¥Î©¥ (7 + 4) √ó 2 = 22cmÏûÖÎãàÎã§."
    ],
    'claude-3': [
      "ÏÇºÍ∞ÅÌòïÏùÄ ÌèâÎ©¥ ÎèÑÌòïÏúºÎ°ú ÏÑ∏ Í∞úÏùò ÏßÅÏÑ†ÏúºÎ°ú ÎëòÎü¨Ïã∏Ïù∏ ÎèÑÌòïÏûÖÎãàÎã§. ÏÇºÍ∞ÅÌòïÏùò ÏÑ∏ ÎÇ¥Í∞ÅÏùò Ìï©ÏùÄ 180ÎèÑÎùºÎäî Ï§ëÏöîÌïú ÏÑ±ÏßàÏù¥ ÏûàÏäµÎãàÎã§.",
      "ÎçßÏÖàÏùÑ Îã®Í≥ÑÎ≥ÑÎ°ú Ìï¥Î≥¥Í≤†ÏäµÎãàÎã§. 17 + 25ÏóêÏÑú Î®ºÏ†Ä ÏùºÏùò ÏûêÎ¶¨Î•º Í≥ÑÏÇ∞ÌïòÎ©¥ 7 + 5 = 12ÏûÖÎãàÎã§. 12ÏóêÏÑú 10ÏùÑ Ïã≠Ïùò ÏûêÎ¶¨Î°ú Ïò¨Î¶¨Í≥† 2Î•º ÎÇ®Í≤®Îë°ÎãàÎã§.",
      "ÏßÅÏÇ¨Í∞ÅÌòïÏùò ÎëòÎ†àÎ•º Íµ¨ÌïòÎäî Í≥µÏãùÏùÑ ÏÇ¨Ïö©Ìï¥Î≥¥Í≤†ÏäµÎãàÎã§. ÎëòÎ†à = 2 √ó (Í∞ÄÎ°ú + ÏÑ∏Î°ú) = 2 √ó (7 + 4) = 2 √ó 11 = 22cmÏûÖÎãàÎã§."
    ],
    'gemini-pro': [
      "ÏÇºÍ∞ÅÌòïÏùÄ Í∏∞ÌïòÌïôÏùò Í∏∞Î≥∏ ÎèÑÌòï Ï§ë ÌïòÎÇòÏûÖÎãàÎã§. ÏÇºÍ∞ÅÌòïÏùò ÌäπÏßïÏúºÎ°úÎäî ÏÑ∏ Í∞úÏùò Î≥Ä, ÏÑ∏ Í∞úÏùò Íº≠ÏßìÏ†ê, Í∑∏Î¶¨Í≥† ÏÑ∏ ÎÇ¥Í∞ÅÏùò Ìï©Ïù¥ 180ÎèÑÎùºÎäî Ï†êÏù¥ ÏûàÏäµÎãàÎã§.",
      "Ïù¥ Î¨∏Ï†úÎäî Îëê ÏûêÎ¶¨ ÏàòÏùò ÎçßÏÖàÏûÖÎãàÎã§. 17 + 25 = 42ÏûÖÎãàÎã§. Í≥ÑÏÇ∞ Í≥ºÏ†ïÏùÑ Î≥¥Î©¥ 7 + 5 = 12Ïù¥ÎØÄÎ°ú ÏùºÏùò ÏûêÎ¶¨Îäî 2, Ïã≠Ïùò ÏûêÎ¶¨Îäî 1 + 1 + 2 = 4ÏûÖÎãàÎã§.",
      "ÏßÅÏÇ¨Í∞ÅÌòï ÎëòÎ†à Í≥ÑÏÇ∞: ÎëòÎ†à = 2 √ó (Í∏∏Ïù¥ + ÎÑàÎπÑ) = 2 √ó (7cm + 4cm) = 2 √ó 11cm = 22cm"
    ]
  };

  const modelResponses = responses[modelId] || responses['gpt-4'];
  const randomIndex = Math.floor(Math.random() * modelResponses.length);
  
  return modelResponses[randomIndex];
} 