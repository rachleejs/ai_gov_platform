// ì‹¤ì œ ëª¨ë¸ API í˜¸ì¶œì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹°

interface ModelConfig {
  provider: 'openai' | 'anthropic' | 'google' | 'huggingface';
  modelName: string;
  apiKey: string;
  endpoint?: string;
}

interface ModelResponse {
  text: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export class ModelAPIClient {
  private config: ModelConfig;

  constructor(config: ModelConfig) {
    this.config = config;
  }

  async generateResponse(prompt: string): Promise<ModelResponse> {
    switch (this.config.provider) {
      case 'openai':
        return this.callOpenAI(prompt);
      case 'anthropic':
        return this.callAnthropic(prompt);
      case 'google':
        return this.callGoogle(prompt);
      case 'huggingface':
        return this.callHuggingFace(prompt);
      default:
        throw new Error(`Unsupported provider: ${this.config.provider}`);
    }
  }

  private async callOpenAI(prompt: string): Promise<ModelResponse> {
    try {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.config.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.config.modelName,
          messages: [
            {
              role: 'user',
              content: prompt
            }
          ],
          max_tokens: 500,
          temperature: 0.7
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`OpenAI API Error Details:`, {
          status: response.status,
          statusText: response.statusText,
          modelName: this.config.modelName,
          error: errorText
        });
        throw new Error(`OpenAI API error: ${response.statusText} - ${errorText}`);
      }

      const data = await response.json();
      
      return {
        text: data.choices[0].message.content,
        usage: data.usage
      };
    } catch (error) {
      console.error('OpenAI API call failed:', error);
      throw error;
    }
  }

  private async callAnthropic(prompt: string): Promise<ModelResponse> {
    try {
      const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
          'x-api-key': this.config.apiKey,
          'Content-Type': 'application/json',
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
          model: this.config.modelName,
          max_tokens: 500,
          messages: [
            {
              role: 'user',
              content: prompt
            }
          ],
          system: "You are a helpful AI assistant."
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`Anthropic API Error Details:`, {
          status: response.status,
          statusText: response.statusText,
          modelName: this.config.modelName,
          error: errorText
        });
        throw new Error(`Anthropic API error: ${response.statusText} - ${errorText}`);
      }

      const data = await response.json();
      
      return {
        text: data.content[0].text,
        usage: data.usage
      };
    } catch (error) {
      console.error('Anthropic API call failed:', error);
      throw error;
    }
  }

  private async callGoogle(prompt: string): Promise<ModelResponse> {
    try {
      const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${this.config.modelName}:generateContent?key=${this.config.apiKey}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [
            {
              parts: [
                {
                  text: prompt
                }
              ]
            }
          ],
          generationConfig: {
            maxOutputTokens: 500,
            temperature: 0.7
          }
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`Google API Error Details:`, {
          status: response.status,
          statusText: response.statusText,
          modelName: this.config.modelName,
          error: errorText
        });
        throw new Error(`Google API error: ${response.statusText} - ${errorText}`);
      }

      const data = await response.json();
      
      return {
        text: data.candidates[0].content.parts[0].text
      };
    } catch (error) {
      console.error('Google API call failed:', error);
      throw error;
    }
  }

  private async callHuggingFace(prompt: string): Promise<ModelResponse> {
    try {
      const response = await fetch(
        this.config.endpoint || `https://api-inference.huggingface.co/models/${this.config.modelName}`,
        {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${this.config.apiKey}`,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            inputs: prompt,
            parameters: {
              max_new_tokens: 500,
              temperature: 0.7,
              return_full_text: false
            }
          })
        }
      );

      if (!response.ok) {
        throw new Error(`HuggingFace API error: ${response.statusText}`);
      }

      const data = await response.json();
      
      return {
        text: Array.isArray(data) ? data[0].generated_text : data.generated_text
      };
    } catch (error) {
      console.error('HuggingFace API call failed:', error);
      throw error;
    }
  }
}

// ëª¨ë¸ ì„¤ì •ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
export async function getModelConfig(modelId: string): Promise<ModelConfig | null> {
  console.log(`ğŸ” Getting config for model: ${modelId}`);
  
  // ì§€ì •ëœ 3ê°œ UUIDì— ëŒ€í•œ ì§ì ‘ ë§¤í•‘
  const uuidToConfigMap: { [key: string]: ModelConfig } = {
    'cb7d2bb8-049c-4271-99a2-bffedebe2487': {
      provider: 'openai',
      modelName: 'gpt-4o',
      apiKey: process.env.OPENAI_API_KEY || '',
    },
    '603d268f-d984-43b6-a85e-445bdd955061': {
      provider: 'anthropic',
      modelName: 'claude-3-opus-20240229',
      apiKey: process.env.ANTHROPIC_API_KEY || '',
    },
    '3e72f00e-b450-4dff-812e-a013c4cca457': {
      provider: 'google',
      modelName: 'gemini-2.0-flash',
      apiKey: process.env.GOOGLE_API_KEY || '',
    }
  };

  // UUID ê¸°ë°˜ ì„¤ì • í™•ì¸
  if (uuidToConfigMap[modelId]) {
    console.log(`âœ… Found direct UUID mapping for ${modelId}`);
    return uuidToConfigMap[modelId];
  }

  // UUID í˜•íƒœì´ì§€ë§Œ ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
  if (modelId.includes('-')) {
    try {
      const response = await fetch(`${process.env.NEXT_PUBLIC_SITE_URL || 'http://localhost:3000'}/api/models`);
      if (response.ok) {
        const models = await response.json();
        const model = models.find((m: any) => m.id === modelId);
        if (model) {
          console.log(`ğŸ“‹ Found model in database: ${model.name}`);
          // ëª¨ë¸ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì • ë§¤í•‘
          const normalizedName = model.name.toLowerCase().replace(/[^a-z0-9]/g, '');
          if (normalizedName.includes('gpt4') || normalizedName.includes('gpt-4') || normalizedName.includes('gpt4')) {
            return {
              provider: 'openai',
              modelName: 'gpt-4o',
              apiKey: process.env.OPENAI_API_KEY || '',
            };
          } else if (normalizedName.includes('claude')) {
            return {
              provider: 'anthropic',
              modelName: 'claude-3-opus-20240229',
              apiKey: process.env.ANTHROPIC_API_KEY || '',
            };
          } else if (normalizedName.includes('gemini') || normalizedName.includes('google')) {
            return {
              provider: 'google',
              modelName: 'gemini-2.0-flash',
              apiKey: process.env.GOOGLE_API_KEY || '',
            };
          }
        }
      }
    } catch (error) {
      console.error('Error fetching model info:', error);
    }
  }
  
  // ë¬¸ìì—´ ê¸°ë°˜ ëª¨ë¸ ID ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
  const modelConfigs: { [key: string]: ModelConfig } = {
    'gpt-4-turbo': {
      provider: 'openai',
      modelName: 'gpt-4o',
      apiKey: process.env.OPENAI_API_KEY || '',
    },
    'claude-3-opus': {
      provider: 'anthropic',
      modelName: 'claude-3-opus-20240229',
      apiKey: process.env.ANTHROPIC_API_KEY || '',
    },
    'gemini-2.0-flash': {
      provider: 'google',
      modelName: 'gemini-2.0-flash',
      apiKey: process.env.GOOGLE_API_KEY || '',
    }
  };

  return modelConfigs[modelId] || null;
}

// ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜
export async function callModel(modelId: string, prompt: string): Promise<string> {
  console.log(`ğŸš€ Calling model: ${modelId}`);
  const config = await getModelConfig(modelId);
  
  if (!config) {
    console.warn(`âŒ No configuration found for model: ${modelId}, using fallback`);
    // ì„¤ì •ì´ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ì‘ë‹µ ë°˜í™˜
    return await getFallbackResponse(modelId, prompt);
  }

  if (!config.apiKey) {
    console.warn(`ğŸ”‘ No API key found for model: ${modelId}, using fallback`);
    return await getFallbackResponse(modelId, prompt);
  }

  console.log(`âœ… API key found for ${config.provider}, attempting real API call...`);
  
  try {
    const client = new ModelAPIClient(config);
    const response = await client.generateResponse(prompt);
    console.log(`âœ… Successfully called ${modelId} API`);
    return response.text;
  } catch (error) {
    console.error(`âŒ Error calling model ${modelId}:`, error);
    // API í˜¸ì¶œ ì‹¤íŒ¨ì‹œ í´ë°± ì‘ë‹µ ë°˜í™˜
    return await getFallbackResponse(modelId, prompt);
  }
}

// í´ë°± ì‘ë‹µ (ì‹œë®¬ë ˆì´ì…˜)
async function getFallbackResponse(modelId: string, prompt: string): Promise<string> {
  console.log(`ğŸ”„ Using fallback simulation for model: ${modelId}`);
  
  // ì‹¤ì œ API í˜¸ì¶œê³¼ ë¹„ìŠ·í•œ ì§€ì—° ì‹œê°„ ì¶”ê°€ (2-4ì´ˆ)
  const delay = 2000 + Math.random() * 2000;
  await new Promise(resolve => setTimeout(resolve, delay));
  
  const responses: { [key: string]: string[] } = {
    'gpt-4': [
      "ì‚¼ê°í˜•ì€ ì„¸ ê°œì˜ ë³€ê³¼ ì„¸ ê°œì˜ ê¼­ì§“ì ì„ ê°€ì§„ ë‹¤ê°í˜•ì…ë‹ˆë‹¤. ì‚¼ê°í˜•ì˜ ë‚´ê°ì˜ í•©ì€ í•­ìƒ 180ë„ì…ë‹ˆë‹¤.",
      "17 + 25ë¥¼ ê³„ì‚°í•˜ë©´ 42ì…ë‹ˆë‹¤. ì¼ì˜ ìë¦¬ 7 + 5 = 12ì´ë¯€ë¡œ 1ì„ ì˜¬ë¦¼í•˜ì—¬ ì‹­ì˜ ìë¦¬ëŠ” 1 + 1 + 2 = 4ê°€ ë©ë‹ˆë‹¤.",
      "ì§ì‚¬ê°í˜•ì˜ ë‘˜ë ˆëŠ” (ê°€ë¡œ + ì„¸ë¡œ) Ã— 2ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ë¡œê°€ 7cm, ì„¸ë¡œê°€ 4cmì´ë©´ (7 + 4) Ã— 2 = 22cmì…ë‹ˆë‹¤."
    ],
    'claude-3': [
      "ì‚¼ê°í˜•ì€ í‰ë©´ ë„í˜•ìœ¼ë¡œ ì„¸ ê°œì˜ ì§ì„ ìœ¼ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë„í˜•ì…ë‹ˆë‹¤. ì‚¼ê°í˜•ì˜ ì„¸ ë‚´ê°ì˜ í•©ì€ 180ë„ë¼ëŠ” ì¤‘ìš”í•œ ì„±ì§ˆì´ ìˆìŠµë‹ˆë‹¤.",
      "ë§ì…ˆì„ ë‹¨ê³„ë³„ë¡œ í•´ë³´ê² ìŠµë‹ˆë‹¤. 17 + 25ì—ì„œ ë¨¼ì € ì¼ì˜ ìë¦¬ë¥¼ ê³„ì‚°í•˜ë©´ 7 + 5 = 12ì…ë‹ˆë‹¤. 12ì—ì„œ 10ì„ ì‹­ì˜ ìë¦¬ë¡œ ì˜¬ë¦¬ê³  2ë¥¼ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.",
      "ì§ì‚¬ê°í˜•ì˜ ë‘˜ë ˆë¥¼ êµ¬í•˜ëŠ” ê³µì‹ì„ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‘˜ë ˆ = 2 Ã— (ê°€ë¡œ + ì„¸ë¡œ) = 2 Ã— (7 + 4) = 2 Ã— 11 = 22cmì…ë‹ˆë‹¤."
    ],
    'gemini-pro': [
      "ì‚¼ê°í˜•ì€ ê¸°í•˜í•™ì˜ ê¸°ë³¸ ë„í˜• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì‚¼ê°í˜•ì˜ íŠ¹ì§•ìœ¼ë¡œëŠ” ì„¸ ê°œì˜ ë³€, ì„¸ ê°œì˜ ê¼­ì§“ì , ê·¸ë¦¬ê³  ì„¸ ë‚´ê°ì˜ í•©ì´ 180ë„ë¼ëŠ” ì ì´ ìˆìŠµë‹ˆë‹¤.",
      "ì´ ë¬¸ì œëŠ” ë‘ ìë¦¬ ìˆ˜ì˜ ë§ì…ˆì…ë‹ˆë‹¤. 17 + 25 = 42ì…ë‹ˆë‹¤. ê³„ì‚° ê³¼ì •ì„ ë³´ë©´ 7 + 5 = 12ì´ë¯€ë¡œ ì¼ì˜ ìë¦¬ëŠ” 2, ì‹­ì˜ ìë¦¬ëŠ” 1 + 1 + 2 = 4ì…ë‹ˆë‹¤.",
      "ì§ì‚¬ê°í˜• ë‘˜ë ˆ ê³„ì‚°: ë‘˜ë ˆ = 2 Ã— (ê¸¸ì´ + ë„ˆë¹„) = 2 Ã— (7cm + 4cm) = 2 Ã— 11cm = 22cm"
    ]
  };

  const modelResponses = responses[modelId] || responses['gpt-4'];
  const randomIndex = Math.floor(Math.random() * modelResponses.length);
  
  return modelResponses[randomIndex];
} 