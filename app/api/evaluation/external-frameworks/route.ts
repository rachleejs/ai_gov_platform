// ì™¸ë¶€ í‰ê°€ í”„ë ˆì„ì›Œí¬ í†µí•© API

import { NextRequest, NextResponse } from 'next/server';
import { cookies } from 'next/headers';
import { createClient } from '@/lib/supabase/server';
import { OpenAIEvalsManager } from '@/lib/openaiEvalsIntegration';
import { HuggingFaceEvaluator } from '@/lib/huggingfaceEvaluator';

interface ExternalFrameworkRequest {
  framework: 'openai-evals' | 'huggingface-evaluate' | 'lm-eval-harness' | 'big-bench';
  modelId: string;
  evaluationId: string;
  options?: Record<string, any>;
}

interface ExternalFrameworkResponse {
  success: boolean;
  data?: {
    framework: string;
    evaluationId: string;
    modelId: string;
    score: number;
    details: any;
    timestamp: Date;
  };
  error?: string;
  availableEvaluations?: Array<{
    id: string;
    name: string;
    description: string;
    framework: string;
    category: string;
  }>;
}

// GET: ì‚¬ìš© ê°€ëŠ¥í•œ ì™¸ë¶€ í‰ê°€ í”„ë ˆì„ì›Œí¬ ë° í‰ê°€ ëª©ë¡ ì¡°íšŒ
export async function GET(request: NextRequest) {
  try {
    console.log("GET /api/evaluation/external-frameworks - ì™¸ë¶€ í”„ë ˆì„ì›Œí¬ ëª©ë¡ ìš”ì²­");
    
    const { searchParams } = new URL(request.url);
    const framework = searchParams.get('framework');
    
    let availableEvaluations: Array<{
      id: string;
      name: string;
      description: string;
      framework: string;
      category: string;
    }> = [];

    // ê° í”„ë ˆì„ì›Œí¬ë³„ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ëª©ë¡ ìˆ˜ì§‘
    if (!framework || framework === 'openai-evals') {
      const openaiEvals = new OpenAIEvalsManager();
      const evals = openaiEvals.getAvailableEvals();
      availableEvaluations.push(...evals.map(evaluation => ({
        id: evaluation.id,
        name: evaluation.name,
        description: evaluation.description,
        framework: 'openai-evals',
        category: evaluation.category
      })));
    }

    if (!framework || framework === 'huggingface-evaluate') {
      const hfEvaluator = new HuggingFaceEvaluator();
      const metrics = hfEvaluator.getAvailableMetrics();
      availableEvaluations.push(...metrics.map(metric => ({
        id: metric.id,
        name: metric.name,
        description: metric.description,
        framework: 'huggingface-evaluate',
        category: metric.category
      })));
    }

    // LM Evaluation Harness ì§€ì› í‰ê°€ë“¤ (í•˜ë“œì½”ë”©)
    if (!framework || framework === 'lm-eval-harness') {
      const lmEvalTasks = [
        {
          id: 'hellaswag',
          name: 'HellaSwag',
          description: 'ìƒì‹ ì¶”ë¡  í‰ê°€',
          framework: 'lm-eval-harness',
          category: 'reasoning'
        },
        {
          id: 'arc_easy',
          name: 'ARC Easy',
          description: 'ê³¼í•™ ì§ˆë¬¸ ë‹µë³€ (ì‰¬ì›€)',
          framework: 'lm-eval-harness',
          category: 'knowledge'
        },
        {
          id: 'arc_challenge',
          name: 'ARC Challenge',
          description: 'ê³¼í•™ ì§ˆë¬¸ ë‹µë³€ (ì–´ë ¤ì›€)',
          framework: 'lm-eval-harness',
          category: 'knowledge'
        },
        {
          id: 'mmlu',
          name: 'MMLU',
          description: 'ëŒ€ê·œëª¨ ë‹¤ì˜ì—­ ì–¸ì–´ ì´í•´',
          framework: 'lm-eval-harness',
          category: 'knowledge'
        }
      ];
      availableEvaluations.push(...lmEvalTasks);
    }

    // BIG-bench ì§€ì› í‰ê°€ë“¤ (í•˜ë“œì½”ë”©)
    if (!framework || framework === 'big-bench') {
      const bigBenchTasks = [
        {
          id: 'arithmetic',
          name: 'Arithmetic',
          description: 'ì‚°ìˆ  ê³„ì‚° ëŠ¥ë ¥',
          framework: 'big-bench',
          category: 'math'
        },
        {
          id: 'logical_deduction',
          name: 'Logical Deduction',
          description: 'ë…¼ë¦¬ì  ì¶”ë¡ ',
          framework: 'big-bench',
          category: 'reasoning'
        },
        {
          id: 'causal_judgement',
          name: 'Causal Judgement',
          description: 'ì¸ê³¼ê´€ê³„ íŒë‹¨',
          framework: 'big-bench',
          category: 'reasoning'
        }
      ];
      availableEvaluations.push(...bigBenchTasks);
    }
    
    return NextResponse.json({
      success: true,
      availableEvaluations
    } as ExternalFrameworkResponse);
    
  } catch (error) {
    console.error('Error fetching external frameworks:', error);
    return NextResponse.json(
      { success: false, error: 'Failed to fetch external frameworks' } as ExternalFrameworkResponse,
      { status: 500 }
    );
  }
}

// POST: ì™¸ë¶€ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰
export async function POST(request: NextRequest) {
  try {
    console.log("POST /api/evaluation/external-frameworks - ì™¸ë¶€ í”„ë ˆì„ì›Œí¬ í‰ê°€ ì‹¤í–‰");
    
    // ì¸ì¦ í™•ì¸
    const cookieStore = cookies();
    const supabase = createClient(cookieStore);
    const { data: { session } } = await supabase.auth.getSession();
    
    // ì¸ì¦ ìš”êµ¬ ì‚¬í•­ ì™„í™” - í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ ì¸ì¦ ì—†ì´ë„ ì ‘ê·¼ í—ˆìš©
    // ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ì´ ë¶€ë¶„ì„ ë‹¤ì‹œ í™œì„±í™”í•´ì•¼ í•©ë‹ˆë‹¤
    /*
    if (!session) {
      return NextResponse.json(
        { success: false, error: 'Unauthorized' } as ExternalFrameworkResponse,
        { status: 401 }
      );
    }
    */
    
    const body: ExternalFrameworkRequest = await request.json();
    const { framework, modelId, evaluationId, options = {} } = body;
    
    if (!framework || !modelId || !evaluationId) {
      return NextResponse.json(
        { success: false, error: 'framework, modelId, and evaluationId are required' } as ExternalFrameworkResponse,
        { status: 400 }
      );
    }
    
    // ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    const { data: model, error: modelError } = await supabase
      .from('ai_models')
      .select('name')
      .eq('id', modelId)
      .single();
    
    if (modelError || !model) {
      return NextResponse.json(
        { success: false, error: 'Model not found' } as ExternalFrameworkResponse,
        { status: 404 }
      );
    }
    
    console.log(`ğŸ”§ Running ${framework} evaluation: ${evaluationId} for model: ${model.name}`);
    
    let result: any;
    
    // í”„ë ˆì„ì›Œí¬ë³„ë¡œ í‰ê°€ ì‹¤í–‰
    switch (framework) {
      case 'openai-evals':
        const openaiManager = new OpenAIEvalsManager();
        result = await openaiManager.runEvaluation(evaluationId, model.name, options);
        break;
        
      case 'huggingface-evaluate':
        const hfEvaluator = new HuggingFaceEvaluator();
        const hfRequest = {
          modelId,
          metricIds: [evaluationId],
          testData: {
            questions: options.questions || ['Sample question'],
            references: options.references
          }
        };
        const hfResult = await hfEvaluator.evaluateModel(hfRequest);
        result = {
          evalId: evaluationId,
          evalName: evaluationId,
          modelName: model.name,
          score: hfResult.metrics[evaluationId]?.score || 0,
          details: hfResult.metrics[evaluationId]?.details || {},
          timestamp: new Date()
        };
        break;
        
      case 'lm-eval-harness':
        result = await runLMEvalHarness(evaluationId, model.name, options);
        break;
        
      case 'big-bench':
        result = await runBigBench(evaluationId, model.name, options);
        break;
        
      default:
        throw new Error(`Unsupported framework: ${framework}`);
    }
    
    // ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì„¸ì…˜ì´ ìˆëŠ” ê²½ìš°ë§Œ)
    if (session) {
      try {
        const { data: savedResult, error: saveError } = await supabase
          .from('external_framework_evaluations')
          .insert([
            {
              model_id: modelId,
              user_id: session.user.id,
              framework: framework,
              evaluation_id: evaluationId,
              score: result.score || 0,
              details: result.details || {},
              timestamp: result.timestamp.toISOString()
            }
          ])
          .select()
          .single();
          
        if (saveError) {
          console.error('Error saving external framework evaluation result:', saveError);
          // ì €ì¥ ì‹¤íŒ¨í•´ë„ í‰ê°€ ê²°ê³¼ëŠ” ë°˜í™˜
        } else {
          console.log('âœ… External framework evaluation result saved to database');
        }
      } catch (dbError) {
        console.error('Database save error:', dbError);
        // ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ê°€ ìˆì–´ë„ í‰ê°€ ê²°ê³¼ëŠ” ë°˜í™˜
      }
    } else {
      console.log('âš ï¸ No session found, skipping database save (test mode)');
    }
    
    return NextResponse.json({
      success: true,
      data: {
        framework,
        evaluationId,
        modelId,
        score: result.score || 0,
        details: result.details || {},
        timestamp: result.timestamp
      }
    } as ExternalFrameworkResponse);
    
  } catch (error) {
    console.error('Error executing external framework evaluation:', error);
    return NextResponse.json(
      { 
        success: false, 
        error: error instanceof Error ? error.message : 'External framework evaluation failed' 
      } as ExternalFrameworkResponse,
      { status: 500 }
    );
  }
}

// LM Evaluation Harness ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
async function runLMEvalHarness(taskName: string, modelName: string, options: any) {
  console.log(`ğŸ”„ Simulating LM Eval Harness: ${taskName} for ${modelName}`);
  
  // ì‹œë®¬ë ˆì´ì…˜ ì§€ì—° ì‹œê°„
  await new Promise(resolve => setTimeout(resolve, 2000 + Math.random() * 3000));
  
  // íƒœìŠ¤í¬ë³„ ì‹œë®¬ë ˆì´ì…˜ ì ìˆ˜
  const simulatedScores: Record<string, number> = {
    'hellaswag': 65 + Math.random() * 20,
    'arc_easy': 70 + Math.random() * 25, 
    'arc_challenge': 45 + Math.random() * 30,
    'mmlu': 55 + Math.random() * 35
  };
  
  const score = Math.round(simulatedScores[taskName] || (50 + Math.random() * 40));
  
  return {
    evalId: taskName,
    evalName: taskName,
    modelName,
    score,
    details: { 
      simulation: true,
      message: `Simulated ${taskName} evaluation for ${modelName}`,
      framework: 'lm-eval-harness' 
    },
    timestamp: new Date()
  };
}

// BIG-bench ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
async function runBigBench(taskName: string, modelName: string, options: any) {
  console.log(`ğŸ”„ Simulating BIG-bench: ${taskName} for ${modelName}`);
  
  // ì‹œë®¬ë ˆì´ì…˜ ì§€ì—° ì‹œê°„
  await new Promise(resolve => setTimeout(resolve, 3000 + Math.random() * 2000));
  
  // íƒœìŠ¤í¬ë³„ ì‹œë®¬ë ˆì´ì…˜ ì ìˆ˜
  const simulatedScores: Record<string, number> = {
    'arithmetic': 75 + Math.random() * 20,
    'logical_deduction': 60 + Math.random() * 30,
    'causal_judgement': 55 + Math.random() * 25
  };
  
  const score = Math.round(simulatedScores[taskName] || (45 + Math.random() * 50));
  
  return {
    evalId: taskName,
    evalName: taskName,
    modelName,
    score,
    details: { 
      simulation: true,
      message: `Simulated ${taskName} evaluation for ${modelName}`,
      framework: 'big-bench' 
    },
    timestamp: new Date()
  };
} 